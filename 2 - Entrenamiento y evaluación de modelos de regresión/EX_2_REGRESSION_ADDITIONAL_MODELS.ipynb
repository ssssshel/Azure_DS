{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression - Experimenting with additional models\n",
    "\n",
    "In the previous notebook, we used simple regression models to look at the relationship between features of a bike rentals dataset. In this notebook, we'll experiment with more complex models to improve our regression performance.\n",
    "\n",
    "Let's start by loading the bicycle sharing data as a **Pandas** DataFrame and viewing the first few rows. We'll also split our data into training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules we'll need for this notebook\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# load the training dataset\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/ml-basics/daily-bike-share.csv\n",
    "bike_data = pd.read_csv('daily-bike-share.csv')\n",
    "bike_data['day'] = pd.DatetimeIndex(bike_data['dteday']).day\n",
    "numeric_features = ['temp', 'atemp', 'hum', 'windspeed']\n",
    "categorical_features = ['season','mnth','holiday','weekday','workingday','weathersit', 'day']\n",
    "bike_data[numeric_features + ['rentals']].describe()\n",
    "print(bike_data.head())\n",
    "\n",
    "\n",
    "# Separate features and labels\n",
    "# After separating the dataset, we now have numpy arrays named **X** containing the features, and **y** containing the labels.\n",
    "X, y = bike_data[['season','mnth', 'holiday','weekday','workingday','weathersit','temp', 'atemp', 'hum', 'windspeed']].values, bike_data['rentals'].values\n",
    "\n",
    "# Split data 70%-30% into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "\n",
    "print ('Training Set: %d rows\\nTest Set: %d rows' % (X_train.shape[0], X_test.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2022-07-15 16:38:05 (4.61 MB/s) - ‘daily-bike-share.csv.2’ saved [48800/48800]\n",
    "\n",
    "   instant    dteday  season  yr  mnth  holiday  weekday  workingday  \\\n",
    "0        1  1/1/2011       1   0     1        0        6           0   \n",
    "1        2  1/2/2011       1   0     1        0        0           0   \n",
    "2        3  1/3/2011       1   0     1        0        1           1   \n",
    "3        4  1/4/2011       1   0     1        0        2           1   \n",
    "4        5  1/5/2011       1   0     1        0        3           1   \n",
    "\n",
    "   weathersit      temp     atemp       hum  windspeed  rentals  day  \n",
    "0           2  0.344167  0.363625  0.805833   0.160446      331    1  \n",
    "1           2  0.363478  0.353739  0.696087   0.248539      131    2  \n",
    "2           1  0.196364  0.189405  0.437273   0.248309      120    3  \n",
    "3           1  0.200000  0.212122  0.590435   0.160296      108    4  \n",
    "4           1  0.226957  0.229270  0.436957   0.186900       82    5  \n",
    "Training Set: 511 rows\n",
    "Test Set: 220 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the following four datasets:\n",
    "\n",
    "- **X_train**: The feature values we'll use to train the model\n",
    "- **y_train**: The corresponding labels we'll use to train the model\n",
    "- **X_test**: The feature values we'll use to validate the model\n",
    "- **y_test**: The corresponding labels we'll use to validate the model\n",
    "\n",
    "Now we're ready to train a model by fitting a suitable regression algorithm to the training data. \n",
    "\n",
    "## Experiment with Algorithms\n",
    "\n",
    "The linear regression algorithm we used last time to train the model has some predictive capability, but there are many kinds of regression algorithm we could try, including:\n",
    "\n",
    "- **Linear algorithms**: Not just the Linear Regression algorithm we used above (which is technically an *Ordinary Least Squares* algorithm), but other variants such as *Lasso* and *Ridge*.\n",
    "- **Tree-based algorithms**: Algorithms that build a decision tree to reach a prediction.\n",
    "- **Ensemble algorithms**: Algorithms that combine the outputs of multiple base algorithms to improve generalizability.\n",
    "\n",
    "> **Note**: For a full list of Scikit-Learn estimators that encapsulate algorithms for supervised machine learning, see the [Scikit-Learn documentation](https://scikit-learn.org/stable/supervised_learning.html). There are many algorithms to choose from, but for most real-world scenarios, the [Scikit-Learn estimator cheat sheet](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html) can help you find a suitable starting point. \n",
    "\n",
    "### Try Another Linear Algorithm\n",
    "\n",
    "Let's try training our regression model by using a **Lasso** algorithm. We can do this by just changing the estimator in the training code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Fit a lasso model on the training set\n",
    "model = Lasso().fit(X_train, y_train)\n",
    "print (model, \"\\n\")\n",
    "\n",
    "# Evaluate the model using the test data\n",
    "predictions = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(\"MSE:\", mse)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(\"R2:\", r2)\n",
    "\n",
    "# Plot predicted vs actual\n",
    "plt.scatter(y_test, predictions)\n",
    "plt.xlabel('Actual Labels')\n",
    "plt.ylabel('Predicted Labels')\n",
    "plt.title('Daily Bike Share Predictions')\n",
    "# overlay the regression line\n",
    "z = np.polyfit(y_test, predictions, 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(y_test,p(y_test), color='magenta')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso() \n",
    "\n",
    "MSE: 201155.70593338404\n",
    "RMSE: 448.5038527519959\n",
    "R2: 0.6056468637824488"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try a Decision Tree Algorithm\n",
    "\n",
    "As an alternative to a linear model, there's a category of algorithms for machine learning that uses a tree-based approach in which the features in the dataset are examined in a series of evaluations, each of which results in a *branch* in a *decision tree* based on the feature value. At the end of each series of branches are leaf-nodes with the predicted label value based on the feature values.\n",
    "\n",
    "It's easiest to see how this works with an example. Let's train a Decision Tree regression model using the bike rental data. After training the model, the code below will print the model definition and a text representation of the tree it uses to predict label values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import export_text\n",
    "\n",
    "# Train the model\n",
    "model = DecisionTreeRegressor().fit(X_train, y_train)\n",
    "print (model, \"\\n\")\n",
    "\n",
    "# Visualize the model tree\n",
    "tree = export_text(model)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DecisionTreeRegressor() \n",
    "\n",
    "|--- feature_6 <= 0.45\n",
    "|   |--- feature_4 <= 0.50\n",
    "|   |   |--- feature_7 <= 0.32\n",
    "|   |   |   |--- feature_8 <= 0.41\n",
    "|   |   |   |   |--- feature_1 <= 2.50\n",
    "|   |   |   |   |   |--- feature_7 <= 0.26\n",
    "|   |   |   |   |   |   |--- value: [317.00]\n",
    "|   |   |   |   |   |--- feature_7 >  0.26\n",
    "|   |   |   |   |   |   |--- feature_7 <= 0.27\n",
    "|   |   |   |   |   |   |   |--- value: [515.00]\n",
    "|   |   |   |   |   |   |--- feature_7 >  0.27\n",
    "|   |   |   |   |   |   |   |--- value: [558.00]\n",
    "|   |   |   |   |--- feature_1 >  2.50\n",
    "|   |   |   |   |   |--- feature_9 <= 0.28\n",
    "|   |   |   |   |   |   |--- feature_8 <= 0.37\n",
    "|   |   |   |   |   |   |   |--- value: [968.00]\n",
    "|   |   |   |   |   |   |--- feature_8 >  0.37\n",
    "|   |   |   |   |   |   |   |--- value: [981.00]\n",
    "|   |   |   |   |   |--- feature_9 >  0.28\n",
    "|   |   |   |   |   |   |--- feature_3 <= 3.00\n",
    "|   |   |   |   |   |   |   |--- value: [710.00]\n",
    "|   |   |   |   |   |   |--- feature_3 >  3.00\n",
    "|   |   |   |   |   |   |   |--- value: [532.00]\n",
    "|   |   |   |--- feature_8 >  0.41\n",
    "|   |   |   |   |--- feature_7 <= 0.25\n",
    "|   |   |   |   |   |--- feature_6 <= 0.18\n",
    "|   |   |   |   |   |   |--- feature_8 <= 0.43\n",
    "|   |   |   |   |   |   |   |--- value: [284.00]\n",
    "|   |   |   |   |   |   |--- feature_8 >  0.43\n",
    "|   |   |   |   |   |   |   |--- feature_8 <= 0.45\n",
    "|   |   |   |   |   |   |   |   |--- value: [150.00]\n",
    "|   |   |   |   |   |   |   |--- feature_8 >  0.45\n",
    "|   |   |   |   |   |   |   |   |--- feature_9 <= 0.21\n",
    "|   |   |   |   |   |   |   |   |   |--- value: [117.00]\n",
    "|   |   |   |   |   |   |   |   |--- feature_9 >  0.21\n",
    "|   |   |   |   |   |   |   |   |   |--- feature_5 <= 1.50\n",
    "|   |   |   |   |   |   |   |   |   |   |--- value: [73.00]\n",
    "|   |   |   |   |   |   |   |   |   |--- feature_5 >  1.50\n",
    "|   |   |   |   |   |   |   |   |   |   |--- feature_9 <= 0.24\n",
    "|   |   |   |   |   |   |   |   |   |   |   |--- value: [67.00]\n",
    "|   |   |   |   |   |   |   |   |   |   |--- feature_9 >  0.24\n",
    "|   |   |   |   |   |   |   |   |   |   |   |--- value: [68.00]\n",
    "|   |   |   |   |   |--- feature_6 >  0.18\n",
    "|   |   |   |   |   |   |--- feature_8 <= 0.61\n",
    "|   |   |   |   |   |   |   |--- feature_7 <= 0.19\n",
    "|   |   |   |   |   |   |   |   |--- value: [333.00]\n",
    "|   |   |   |   |   |   |   |--- feature_7 >  0.19\n",
    "|   |   |   |   |   |   |   |   |--- feature_8 <= 0.53\n",
    "|   |   |   |   |   |   |   |   |   |--- feature_9 <= 0.21\n",
    "|   |   |   |   |   |   |   |   |   |   |--- value: [251.00]\n",
    "|   |   |   |   |   |   |   |   |   |--- feature_9 >  0.21\n",
    "|   |   |   |   |   |   |   |   |   |   |--- feature_7 <= 0.21\n",
    "|   |   |   |   |   |   |   |   |   |   |   |--- value: [217.00]\n",
    "|   |   |   |   |   |   |   |   |   |   |--- feature_7 >  0.21\n",
    "|   |   |   |   |   |   |   |   |   |   |   |--- value: [205.00]\n",
    "|   |   |   |   |   |   |   |   |--- feature_8 >  0.53\n",
    "|   |   |   |   |   |   |   |   |   |--- feature_8 <= 0.55\n",
    "|   |   |   |   |   |   |   |   |   |   |--- value: [288.00]\n",
    "|   |   |   |   |   |   |   |   |   |--- feature_8 >  0.55\n",
    "|   |   |   |   |   |   |   |   |   |   |--- value: [275.00]\n",
    "|   |   |   |   |   |   |--- feature_8 >  0.61\n",
    "|   |   |   |   |   |   |   |--- feature_6 <= 0.21\n",
    "|   |   |   |   |   |   |   |   |--- value: [123.00]\n",
    "|   |   |   |   |   |   |   |--- feature_6 >  0.21\n",
    "|   |   |   |   |   |   |   |   |--- value: [140.00]\n",
    "|   |   |   |   |--- feature_7 >  0.25\n",
    "|   |   |   |   |   |--- feature_9 <= 0.11\n",
    "|   |   |   |   |   |   |--- value: [706.00]\n",
    "|   |   |   |   |   |--- feature_9 >  0.11\n",
    "|   |   |   |   |   |   |--- feature_8 <= 0.54\n",
    "|   |   |   |   |   |   |   |--- feature_5 <= 1.50\n",
    "|   |   |   |   |   |   |   |   |--- feature_7 <= 0.26\n",
    "|   |   |   |   |   |   |   |   |   |--- value: [309.00]\n",
    "|   |   |   |   |   |   |   |   |--- feature_7 >  0.26\n",
    "|   |   |   |   |   |   |   |   |   |--- feature_0 <= 2.50\n",
    "|   |   |   |   |   |   |   |   |   |   |--- feature_9 <= 0.16\n",
    "|   |   |   |   |   |   |   |   |   |   |   |--- value: [408.00]\n",
    "|   |   |   |   |   |   |   |   |   |   |--- feature_9 >  0.16\n",
    "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 2\n",
    "|   |   |   |   |   |   |   |   |   |--- feature_0 >  2.50\n",
    "|   |   |   |   |   |   |   |   |   |   |--- feature_9 <= 0.27\n",
    "|   |   |   |   |   |   |   |   |   |   |   |--- value: [502.00]\n",
    "|   |   |   |   |   |   |   |   |   |   |--- feature_9 >  0.27\n",
    "|   |   |   |   |   |   |   |   |   |   |   |--- value: [440.00]\n",
    "|   |   |   |   |   |   |   |--- feature_5 >  1.50\n",
    "|   |   |   |   |   |   |   |   |--- value: [618.00]\n",
    "|   |   |   |   |   |   |--- feature_8 >  0.54\n",
    "|   |   |   |   |   |   |   |--- feature_3 <= 0.50\n",
    "|   |   |   |   |   |   |   |   |--- feature_7 <= 0.28\n",
    "|   |   |   |   |   |   |   |   |   |--- value: [318.00]\n",
    "|   |   |   |   |   |   |   |   |--- feature_7 >  0.28\n",
    "|   |   |   |   |   |   |   |   |   |--- value: [354.00]\n",
    "|   |   |   |   |   |   |   |--- feature_3 >  0.50\n",
    "|   |   |   |   |   |   |   |   |--- feature_5 <= 1.50\n",
    "|   |   |   |   |   |   |   |   |   |--- value: [155.00]\n",
    "|   |   |   |   |   |   |   |   |--- feature_5 >  1.50\n",
    "|   |   |   |   |   |   |   |   |   |--- value: [195.00]\n",
    "|   |   |--- feature_7 >  0.32\n",
    "|   |   |   |--- feature_9 <= 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a tree-based model; but is it any good? Let's evaluate it with the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the test data\n",
    "predictions = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(\"MSE:\", mse)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(\"R2:\", r2)\n",
    "\n",
    "# Plot predicted vs actual\n",
    "plt.scatter(y_test, predictions)\n",
    "plt.xlabel('Actual Labels')\n",
    "plt.ylabel('Predicted Labels')\n",
    "plt.title('Daily Bike Share Predictions')\n",
    "# overlay the regression line\n",
    "z = np.polyfit(y_test, predictions, 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(y_test,p(y_test), color='magenta')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menú de salida\n",
    "MSE: 223636.94090909092\n",
    "RMSE: 472.90267593775667\n",
    "R2: 0.5615738136167744"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tree-based model doesn't seem to have improved over the linear model, so what else could we try?\n",
    "\n",
    "### Try an Ensemble Algorithm\n",
    "\n",
    "Ensemble algorithms work by combining multiple base estimators to produce an optimal model, either by applying an aggregate function to a collection of base models (sometimes referred to a *bagging*) or by building a sequence of models that build on one another to improve predictive performance (referred to as *boosting*).\n",
    "\n",
    "For example, let's try a Random Forest model, which applies an averaging function to multiple Decision Tree models for a better overall model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Train the model\n",
    "model = RandomForestRegressor().fit(X_train, y_train)\n",
    "print (model, \"\\n\")\n",
    "\n",
    "# Evaluate the model using the test data\n",
    "predictions = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(\"MSE:\", mse)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(\"R2:\", r2)\n",
    "\n",
    "# Plot predicted vs actual\n",
    "plt.scatter(y_test, predictions)\n",
    "plt.xlabel('Actual Labels')\n",
    "plt.ylabel('Predicted Labels')\n",
    "plt.title('Daily Bike Share Predictions')\n",
    "# overlay the regression line\n",
    "z = np.polyfit(y_test, predictions, 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(y_test,p(y_test), color='magenta')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForestRegressor() \n",
    "\n",
    "MSE: 106956.51704454546\n",
    "RMSE: 327.0420722851197\n",
    "R2: 0.7903184613147857"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For good measure, let's also try a boosting ensemble algorithm. We'll use a Gradient Boosting estimator, which like a Random Forest algorithm builds multiple trees, but instead of building them all independently and taking the average result, each tree is built on the outputs of the previous one in an attempt to incrementally reduce the loss (error) in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Fit a lasso model on the training set\n",
    "model = GradientBoostingRegressor().fit(X_train, y_train)\n",
    "print (model, \"\\n\")\n",
    "\n",
    "# Evaluate the model using the test data\n",
    "predictions = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(\"MSE:\", mse)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(\"R2:\", r2)\n",
    "\n",
    "# Plot predicted vs actual\n",
    "plt.scatter(y_test, predictions)\n",
    "plt.xlabel('Actual Labels')\n",
    "plt.ylabel('Predicted Labels')\n",
    "plt.title('Daily Bike Share Predictions')\n",
    "# overlay the regression line\n",
    "z = np.polyfit(y_test, predictions, 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(y_test,p(y_test), color='magenta')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GradientBoostingRegressor() \n",
    "\n",
    "MSE: 103890.21811634197\n",
    "RMSE: 322.3200554050926\n",
    "R2: 0.7963297479114384"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Here we've tried a number of new regression algorithms to improve performance. In our notebook we'll look at 'tuning' these algorithms to improve performance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f13baad3d70416a794a9ad2b29e7f938b0cb4bb83d2e11d388b95830c728084f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
